# Delta Lake Table with PySpark on Microsoft Fabric

## ðŸ“Œ Overview
This notebook demonstrates how to build a basic data pipeline using **PySpark** or **SQL** and **Delta Lake** within a Microsoft Fabric environment. It covers how to ingest structured CSV data, define a custom schema, and persist the data as a managed Delta Lake table.

## ðŸš€ What This Project Shows
- Loading CSV data with a defined schema using `StructType`
- Displaying data in a Spark DataFrame
- Writing the data as a **Delta Lake table** using `saveAsTable()`

## ðŸ”§ Technologies Used
- PySpark (Spark SQL)
- Delta Lake
- Microsoft Fabric (notebook interface)
- CSV Data ingestion

---
**Author:** AdriÃ¡n SÃ¡nchez Badillo  
**Curse** AI Skills Fest Challenge: Become a Certified Fabric Data Engineer: Prep for the DP-700 CertificationAI Skills Fest Challenge: Become a Certified Fabric Data Engineer: Prep for the DP-700 Certification

